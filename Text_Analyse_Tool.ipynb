{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQl8trqsAW4se+kfWe8kgE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricLe1404/text_analyse_tool/blob/main/Text_Analyse_Tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📘 Text Analysis Tool - Complete Python Code\n",
        "\n",
        "# Section 1: Imports and Setup\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import warnings\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Download necessary resources upfront\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Section 2: Input Handling with Toggle Timeout\n",
        "def auto_yes(prompt):\n",
        "    print(f\"{prompt} (auto-yes in Colab simulated)\")\n",
        "    time.sleep(1)  # Simulate delay for better UX\n",
        "    return True\n",
        "\n",
        "def get_user_input():\n",
        "    display(Markdown(\"## 👋 Welcome to the Text Insight Tool!\"))\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Type or paste text\")\n",
        "    print(\"2. Upload a text file\")\n",
        "    method = input(\"Enter 1 or 2: \")\n",
        "\n",
        "    if method == '1':\n",
        "        text = input(\"\\nPlease paste your text below:\\n\")\n",
        "    elif method == '2':\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"⚠️ No file uploaded.\")\n",
        "            return None, None, None, None\n",
        "        file = next(iter(uploaded.values()))\n",
        "        try:\n",
        "            text = io.StringIO(file.decode('utf-8')).read()\n",
        "        except:\n",
        "            print(\"❗️ Invalid or unreadable text file.\")\n",
        "            return None, None, None, None\n",
        "    else:\n",
        "        print(\"⚠️ Invalid input choice.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    if not text.strip():\n",
        "        print(\"⚠️ No input detected. Please enter or upload text.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(\"\\n--- Optional Feature Toggles (auto 'yes' after 3 seconds) ---\")\n",
        "    enable_spelling = auto_yes(\"Enable spelling check?\")\n",
        "    auto_correct = auto_yes(\"Enable auto-correction?\")\n",
        "    enable_formality = auto_yes(\"Enable formality detection?\")\n",
        "    enable_sentiment = auto_yes(\"Enable sentiment analysis?\")\n",
        "\n",
        "    return text, enable_spelling, auto_correct, enable_formality, enable_sentiment\n",
        "\n",
        "# Section 3: Clean Text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n",
        "    words = [word for word in words if word not in stop_words and word not in personal_pronouns]\n",
        "    return words\n",
        "\n",
        "# Section 4: Spelling\n",
        "def check_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected = str(blob.correct())\n",
        "    original_words = set(text.split())\n",
        "    corrected_words = set(corrected.split())\n",
        "    misspelled = original_words - corrected_words\n",
        "    return corrected, list(misspelled)\n",
        "\n",
        "# Section 5: Stats\n",
        "def get_statistics(words, text):\n",
        "    total_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    sentence_count = len(nltk.sent_tokenize(text))\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words\n",
        "    word_freq = Counter(words).most_common(5)\n",
        "    return total_words, unique_words, sentence_count, avg_word_length, word_freq\n",
        "\n",
        "# Section 6: Language, Formality, Sentiment\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def detect_formality(text):\n",
        "    informal_words = [\"lol\", \"btw\", \"gonna\", \"wanna\", \"u\", \"idk\", \"bro\", \"dunno\"]\n",
        "    contractions = [\"don't\", \"can't\", \"i'm\", \"you're\", \"won't\"]\n",
        "    text_lower = text.lower()\n",
        "    found = sum(word in text_lower for word in informal_words + contractions)\n",
        "    if found >= 3:\n",
        "        return \"Informal\"\n",
        "    elif found == 0:\n",
        "        return \"Formal\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.05:\n",
        "        sentiment = \"Positive 😊\"\n",
        "    elif polarity < -0.05:\n",
        "        sentiment = \"Negative 😞\"\n",
        "    else:\n",
        "        sentiment = \"Neutral 😐\"\n",
        "    return sentiment, polarity, blob.sentiment.subjectivity\n",
        "\n",
        "# Section 7: Visuals\n",
        "def generate_bar_chart(word_freq):\n",
        "    words, counts = zip(*word_freq)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(\"Top Frequent Words\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_wordcloud(words):\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Word Cloud\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_random_extra_charts(words):\n",
        "    choices = [\"line\", \"pie\", \"hist\"]\n",
        "    selected = random.sample(choices, k=2)\n",
        "\n",
        "    if \"line\" in selected:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(range(len(words[:10])), [len(w) for w in words[:10]], marker='o')\n",
        "        plt.title(\"Length of First 10 Words\")\n",
        "        plt.xlabel(\"Word Position\")\n",
        "        plt.ylabel(\"Word Length\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    if \"pie\" in selected:\n",
        "        freq = Counter(words).most_common(4)\n",
        "        labels, sizes = zip(*freq)\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "        plt.axis('equal')\n",
        "        plt.title(\"Most Common Words (Top 4)\")\n",
        "        plt.show()\n",
        "\n",
        "    if \"hist\" in selected:\n",
        "        lengths = [len(w) for w in words]\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.hist(lengths, bins=range(1, max(lengths)+1), color='coral', edgecolor='black')\n",
        "        plt.title(\"Distribution of Word Lengths\")\n",
        "        plt.xlabel(\"Word Length\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "\n",
        "# Section 8: Summary with Save Option\n",
        "def display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq):\n",
        "    summary = f\"\"\"\n",
        "# 🧾 Summary Dashboard\n",
        "\n",
        "## 📊 Text Statistics\n",
        "- **Total Words**: {total_words}\n",
        "- **Unique Words**: {unique_words}\n",
        "- **Sentences**: {sentence_count}\n",
        "- **Average Word Length**: {avg_word_length:.2f}\n",
        "\n",
        "## 💬 Language & Style\n",
        "- **Language**: {lang}\n",
        "- **Formality**: {formality}\n",
        "- **Spelling Issues**: {', '.join(spelling_issues) if spelling_issues else 'None'}\n",
        "\n",
        "## 😊 Sentiment\n",
        "- **Sentiment**: {sentiment}\n",
        "- **Polarity Score**: {polarity:.2f}\n",
        "- **Subjectivity**: {subjectivity:.2f}\n",
        "\n",
        "## 🔍 Top Words\n",
        "- {', '.join([f'{w} ({c})' for w, c in word_freq])}\n",
        "\"\"\"\n",
        "    display(Markdown(summary))\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"summary_{timestamp}.txt\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(summary)\n",
        "    print(f\"\\n📄 Summary saved as {filename}\")\n",
        "    display(HTML(f'<a download=\"{filename}\" href=\"files/{filename}\" target=\"_blank\">📥 Download Summary</a>'))\n",
        "    display(HTML(f'<button onclick=\"google.colab.kernel.invokeFunction(\\'notebook.exit\\', [], {{}})\">❌ Exit Tool</button>'))\n",
        "\n",
        "# Section 9: Main Runner with Return Option\n",
        "def run_text_analysis():\n",
        "    while True:\n",
        "        user_input = get_user_input()\n",
        "        if not user_input or any(v is None for v in user_input):\n",
        "            break\n",
        "\n",
        "        text, enable_spelling, auto_correct, enable_formality, enable_sentiment = user_input\n",
        "        words = clean_text(text)\n",
        "\n",
        "        if enable_spelling:\n",
        "            corrected_text, spelling_issues = check_spelling(text)\n",
        "            if auto_correct:\n",
        "                text = corrected_text\n",
        "        else:\n",
        "            spelling_issues = []\n",
        "\n",
        "        total_words, unique_words, sentence_count, avg_word_length, word_freq = get_statistics(words, text)\n",
        "        lang = detect_language(text)\n",
        "        formality = detect_formality(text) if enable_formality else 'N/A'\n",
        "        sentiment, polarity, subjectivity = analyze_sentiment(text) if enable_sentiment else ('N/A', 0.0, 0.0)\n",
        "\n",
        "        display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                        lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq)\n",
        "\n",
        "        generate_bar_chart(word_freq)\n",
        "        print(\"\\n--- Additional Charts ---\\n\")\n",
        "        generate_random_extra_charts(words)\n",
        "        print(\"\\n--- Word Cloud ---\\n\")\n",
        "        generate_wordcloud(words)\n",
        "        break\n",
        "\n",
        "# Run the tool\n",
        "run_text_analysis()\n"
      ],
      "metadata": {
        "id": "EVMSmvqgZfAZ",
        "outputId": "2d9089b3-7fed-4153-bc6d-114b4633f8a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-bd10b199e1c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Section 1: Imports and Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2476\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📘 Text Analysis Tool - Complete Python Code\n",
        "\n",
        "# Section 1: Imports and Setup\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import warnings\n",
        "from google.colab import files\n",
        "import io\n",
        "from datetime import datetime\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Section 2: Input Handling with Toggle Timeout\n",
        "def auto_yes(prompt):\n",
        "    print(f\"{prompt} (auto-yes in Colab simulated)\")\n",
        "    time.sleep(1)  # brief pause for UX\n",
        "    return True  # Always returns yes\n",
        "\n",
        "def get_user_input():\n",
        "    display(Markdown(\"## 👋 Welcome to the Text Insight Tool!\"))\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Type or paste text\")\n",
        "    print(\"2. Upload a text file\")\n",
        "    method = input(\"Enter 1 or 2: \")\n",
        "\n",
        "    if method == '1':\n",
        "        text = input(\"\\nPlease paste your text below:\\n\")\n",
        "    elif method == '2':\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"⚠️ No file uploaded.\")\n",
        "            return None, None, None, None\n",
        "        file = next(iter(uploaded.values()))\n",
        "        try:\n",
        "            text = io.StringIO(file.decode('utf-8')).read()\n",
        "        except:\n",
        "            print(\"❗️ Invalid or unreadable text file.\")\n",
        "            return None, None, None, None\n",
        "    else:\n",
        "        print(\"⚠️ Invalid input choice.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    if not text.strip():\n",
        "        print(\"⚠️ No input detected. Please enter or upload text.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(\"\\n--- Optional Feature Toggles (auto 'yes' after 3 seconds) ---\")\n",
        "    enable_spelling = auto_yes(\"Enable spelling check?\")\n",
        "    auto_correct = auto_yes(\"Enable auto-correction?\")\n",
        "    enable_formality = auto_yes(\"Enable formality detection?\")\n",
        "    enable_sentiment = auto_yes(\"Enable sentiment analysis?\")\n",
        "\n",
        "    return text, enable_spelling, auto_correct, enable_formality, enable_sentiment\n",
        "\n",
        "# Section 3: Clean Text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n",
        "    words = [word for word in words if word not in stop_words and word not in personal_pronouns]\n",
        "    return words\n",
        "\n",
        "# Section 4: Spelling\n",
        "def check_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected = str(blob.correct())\n",
        "    original_words = set(text.split())\n",
        "    corrected_words = set(corrected.split())\n",
        "    misspelled = original_words - corrected_words\n",
        "    return corrected, list(misspelled)\n",
        "\n",
        "# Section 5: Stats\n",
        "def get_statistics(words, text):\n",
        "    total_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    sentence_count = len(nltk.sent_tokenize(text))\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words\n",
        "    word_freq = Counter(words).most_common(5)\n",
        "    return total_words, unique_words, sentence_count, avg_word_length, word_freq\n",
        "\n",
        "# Section 6: Language, Formality, Sentiment\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def detect_formality(text):\n",
        "    informal_words = [\"lol\", \"btw\", \"gonna\", \"wanna\", \"u\", \"idk\", \"bro\", \"dunno\"]\n",
        "    contractions = [\"don't\", \"can't\", \"i'm\", \"you're\", \"won't\"]\n",
        "    text_lower = text.lower()\n",
        "    found = sum(word in text_lower for word in informal_words + contractions)\n",
        "    if found >= 3:\n",
        "        return \"Informal\"\n",
        "    elif found == 0:\n",
        "        return \"Formal\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.05:\n",
        "        sentiment = \"Positive 😊\"\n",
        "    elif polarity < -0.05:\n",
        "        sentiment = \"Negative 😞\"\n",
        "    else:\n",
        "        sentiment = \"Neutral 😐\"\n",
        "    return sentiment, polarity, blob.sentiment.subjectivity\n",
        "\n",
        "# Section 7: Visuals\n",
        "def generate_bar_chart(word_freq):\n",
        "    words, counts = zip(*word_freq)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(\"Top Frequent Words\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_wordcloud(words):\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Word Cloud\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_random_extra_charts(words):\n",
        "    choices = [\"line\", \"pie\", \"hist\"]\n",
        "    selected = random.sample(choices, k=2)\n",
        "\n",
        "    if \"line\" in selected:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(range(len(words[:10])), [len(w) for w in words[:10]], marker='o')\n",
        "        plt.title(\"Length of First 10 Words\")\n",
        "        plt.xlabel(\"Word Position\")\n",
        "        plt.ylabel(\"Word Length\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    if \"pie\" in selected:\n",
        "        freq = Counter(words).most_common(4)\n",
        "        labels, sizes = zip(*freq)\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "        plt.axis('equal')\n",
        "        plt.title(\"Most Common Words (Top 4)\")\n",
        "        plt.show()\n",
        "\n",
        "    if \"hist\" in selected:\n",
        "        lengths = [len(w) for w in words]\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.hist(lengths, bins=range(1, max(lengths)+1), color='coral', edgecolor='black')\n",
        "        plt.title(\"Distribution of Word Lengths\")\n",
        "        plt.xlabel(\"Word Length\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "\n",
        "# Section 8: Summary with Save Option\n",
        "def display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq):\n",
        "    summary = f\"\"\"\n",
        "# 🧾 Summary Dashboard\n",
        "\n",
        "## 📊 Text Statistics\n",
        "- **Total Words**: {total_words}\n",
        "- **Unique Words**: {unique_words}\n",
        "- **Sentences**: {sentence_count}\n",
        "- **Average Word Length**: {avg_word_length:.2f}\n",
        "\n",
        "## 💬 Language & Style\n",
        "- **Language**: {lang}\n",
        "- **Formality**: {formality}\n",
        "- **Spelling Issues**: {', '.join(spelling_issues) if spelling_issues else 'None'}\n",
        "\n",
        "## 😊 Sentiment\n",
        "- **Sentiment**: {sentiment}\n",
        "- **Polarity Score**: {polarity:.2f}\n",
        "- **Subjectivity**: {subjectivity:.2f}\n",
        "\n",
        "## 🔍 Top Words\n",
        "- {', '.join([f'{w} ({c})' for w, c in word_freq])}\n",
        "\"\"\"\n",
        "    display(Markdown(summary))\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"summary_{timestamp}.txt\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(summary)\n",
        "    print(f\"\\n📄 Summary saved as {filename}\")\n",
        "    display(HTML(f'<a download=\"{filename}\" href=\"files/{filename}\" target=\"_blank\">📥 Download Summary</a>'))\n",
        "    display(HTML(f'<button onclick=\"google.colab.kernel.invokeFunction(\\'notebook.exit\\', [], {{}})\">❌ Exit Tool</button>'))\n",
        "\n",
        "# Section 9: Main Runner with Return Option\n",
        "def run_text_analysis():\n",
        "    while True:\n",
        "        user_input = get_user_input()\n",
        "        if not user_input or any(v is None for v in user_input):\n",
        "            break\n",
        "\n",
        "        text, enable_spelling, auto_correct, enable_formality, enable_sentiment = user_input\n",
        "        words = clean_text(text)\n",
        "\n",
        "        if enable_spelling:\n",
        "            corrected_text, spelling_issues = check_spelling(text)\n",
        "            if auto_correct:\n",
        "                text = corrected_text\n",
        "        else:\n",
        "            spelling_issues = []\n",
        "\n",
        "        total_words, unique_words, sentence_count, avg_word_length, word_freq = get_statistics(words, text)\n",
        "        lang = detect_language(text)\n",
        "        formality = detect_formality(text) if enable_formality else 'N/A'\n",
        "        sentiment, polarity, subjectivity = analyze_sentiment(text) if enable_sentiment else ('N/A', 0.0, 0.0)\n",
        "\n",
        "        display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                        lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq)\n",
        "\n",
        "        generate_bar_chart(word_freq)\n",
        "        print(\"\\n--- Additional Charts ---\\n\")\n",
        "        generate_random_extra_charts(words)\n",
        "        print(\"\\n--- Word Cloud ---\\n\")\n",
        "        generate_wordcloud(words)\n",
        "        break\n",
        "\n",
        "# Run the tool\n",
        "run_text_analysis()\n"
      ],
      "metadata": {
        "id": "2n7k2mXyYz47",
        "outputId": "8244e2a3-fede-44fc-c4b3-1ffb3752f9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f0a94e25edea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Section 1: Imports and Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2476\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📘 Text Analysis Tool - Complete Python Code\n",
        "\n",
        "# Section 1: Imports and Setup\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from IPython.display import display, Markdown\n",
        "import warnings\n",
        "from google.colab import files\n",
        "import io\n",
        "from datetime import datetime\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Section 2: Input Handling with Toggle Timeout\n",
        "def auto_yes(prompt):\n",
        "    print(prompt + \" (default: y in 3s)\", end=' ', flush=True)\n",
        "    start = time.time()\n",
        "    answer = ''\n",
        "    while time.time() - start < 3:\n",
        "        if answer := input():\n",
        "            break\n",
        "    return (answer or 'y').lower() == 'y'\n",
        "\n",
        "def get_user_input():\n",
        "    display(Markdown(\"## 👋 Welcome to the Text Insight Tool!\"))\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Type or paste text\")\n",
        "    print(\"2. Upload a text file\")\n",
        "    method = input(\"Enter 1 or 2: \")\n",
        "\n",
        "    if method == '1':\n",
        "        text = input(\"\\nPlease paste your text below:\\n\")\n",
        "    elif method == '2':\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"⚠️ No file uploaded.\")\n",
        "            return None, None, None, None\n",
        "        file = next(iter(uploaded.values()))\n",
        "        try:\n",
        "            text = io.StringIO(file.decode('utf-8')).read()\n",
        "        except:\n",
        "            print(\"❗️ Invalid or unreadable text file.\")\n",
        "            return None, None, None, None\n",
        "    else:\n",
        "        print(\"⚠️ Invalid input choice.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    if not text.strip():\n",
        "        print(\"⚠️ No input detected. Please enter or upload text.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(\"\\n--- Optional Feature Toggles (auto 'yes' after 3 seconds) ---\")\n",
        "    enable_spelling = auto_yes(\"Enable spelling check?\")\n",
        "    auto_correct = auto_yes(\"Enable auto-correction?\")\n",
        "    enable_formality = auto_yes(\"Enable formality detection?\")\n",
        "    enable_sentiment = auto_yes(\"Enable sentiment analysis?\")\n",
        "\n",
        "    return text, enable_spelling, auto_correct, enable_formality, enable_sentiment\n",
        "\n",
        "# Section 3: Clean Text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n",
        "    words = [word for word in words if word not in stop_words and word not in personal_pronouns]\n",
        "    return words\n",
        "\n",
        "# Section 4: Spelling\n",
        "def check_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected = str(blob.correct())\n",
        "    original_words = set(text.split())\n",
        "    corrected_words = set(corrected.split())\n",
        "    misspelled = original_words - corrected_words\n",
        "    return corrected, list(misspelled)\n",
        "\n",
        "# Section 5: Stats\n",
        "def get_statistics(words, text):\n",
        "    total_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    sentence_count = len(nltk.sent_tokenize(text))\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words\n",
        "    word_freq = Counter(words).most_common(5)\n",
        "    return total_words, unique_words, sentence_count, avg_word_length, word_freq\n",
        "\n",
        "# Section 6: Language, Formality, Sentiment\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def detect_formality(text):\n",
        "    informal_words = [\"lol\", \"btw\", \"gonna\", \"wanna\", \"u\", \"idk\", \"bro\", \"dunno\"]\n",
        "    contractions = [\"don't\", \"can't\", \"i'm\", \"you're\", \"won't\"]\n",
        "    text_lower = text.lower()\n",
        "    found = sum(word in text_lower for word in informal_words + contractions)\n",
        "    if found >= 3:\n",
        "        return \"Informal\"\n",
        "    elif found == 0:\n",
        "        return \"Formal\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.05:\n",
        "        sentiment = \"Positive 😊\"\n",
        "    elif polarity < -0.05:\n",
        "        sentiment = \"Negative 😞\"\n",
        "    else:\n",
        "        sentiment = \"Neutral 😐\"\n",
        "    return sentiment, polarity, blob.sentiment.subjectivity\n",
        "\n",
        "# Section 7: Visuals\n",
        "def generate_bar_chart(word_freq):\n",
        "    words, counts = zip(*word_freq)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(\"Top Frequent Words\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_wordcloud(words):\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Word Cloud\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_random_extra_charts(words):\n",
        "    choices = [\"line\", \"pie\", \"hist\"]\n",
        "    selected = random.sample(choices, k=2)\n",
        "\n",
        "    if \"line\" in selected:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(range(len(words[:10])), [len(w) for w in words[:10]], marker='o')\n",
        "        plt.title(\"Length of First 10 Words\")\n",
        "        plt.xlabel(\"Word Position\")\n",
        "        plt.ylabel(\"Word Length\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    if \"pie\" in selected:\n",
        "        freq = Counter(words).most_common(4)\n",
        "        labels, sizes = zip(*freq)\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "        plt.axis('equal')\n",
        "        plt.title(\"Most Common Words (Top 4)\")\n",
        "        plt.show()\n",
        "\n",
        "    if \"hist\" in selected:\n",
        "        lengths = [len(w) for w in words]\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.hist(lengths, bins=range(1, max(lengths)+1), color='coral', edgecolor='black')\n",
        "        plt.title(\"Distribution of Word Lengths\")\n",
        "        plt.xlabel(\"Word Length\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "\n",
        "# Section 8: Summary with Save Option\n",
        "def display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq):\n",
        "    summary = f\"\"\"\n",
        "# 🧾 Summary Dashboard\n",
        "\n",
        "## 📊 Text Statistics\n",
        "- **Total Words**: {total_words}\n",
        "- **Unique Words**: {unique_words}\n",
        "- **Sentences**: {sentence_count}\n",
        "- **Average Word Length**: {avg_word_length:.2f}\n",
        "\n",
        "## 💬 Language & Style\n",
        "- **Language**: {lang}\n",
        "- **Formality**: {formality}\n",
        "- **Spelling Issues**: {', '.join(spelling_issues) if spelling_issues else 'None'}\n",
        "\n",
        "## 😊 Sentiment\n",
        "- **Sentiment**: {sentiment}\n",
        "- **Polarity Score**: {polarity:.2f}\n",
        "- **Subjectivity**: {subjectivity:.2f}\n",
        "\n",
        "## 🔍 Top Words\n",
        "- {', '.join([f'{w} ({c})' for w, c in word_freq])}\n",
        "\"\"\"\n",
        "    display(Markdown(summary))\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    with open(f\"summary_{timestamp}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(summary)\n",
        "    print(f\"\\n📄 Summary saved as summary_{timestamp}.txt\")\n",
        "\n",
        "# Section 9: Main Runner with Return Option\n",
        "def run_text_analysis():\n",
        "    while True:\n",
        "        user_input = get_user_input()\n",
        "        if not user_input or any(v is None for v in user_input):\n",
        "            break\n",
        "\n",
        "        text, enable_spelling, auto_correct, enable_formality, enable_sentiment = user_input\n",
        "        words = clean_text(text)\n",
        "\n",
        "        if enable_spelling:\n",
        "            corrected_text, spelling_issues = check_spelling(text)\n",
        "            if auto_correct:\n",
        "                text = corrected_text\n",
        "        else:\n",
        "            spelling_issues = []\n",
        "\n",
        "        total_words, unique_words, sentence_count, avg_word_length, word_freq = get_statistics(words, text)\n",
        "        lang = detect_language(text)\n",
        "        formality = detect_formality(text) if enable_formality else 'N/A'\n",
        "        sentiment, polarity, subjectivity = analyze_sentiment(text) if enable_sentiment else ('N/A', 0.0, 0.0)\n",
        "\n",
        "        display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                        lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq)\n",
        "\n",
        "        generate_bar_chart(word_freq)\n",
        "        print(\"\\n--- Additional Charts ---\\n\")\n",
        "        generate_random_extra_charts(words)\n",
        "        print(\"\\n--- Word Cloud ---\\n\")\n",
        "        generate_wordcloud(words)\n",
        "\n",
        "        choice = input(\"\\nType 'exit' to quit or press Enter to analyze another text: \").lower()\n",
        "        if choice == 'exit':\n",
        "            print(\"Goodbye! 👋\")\n",
        "            break\n",
        "\n",
        "# Run the tool\n",
        "run_text_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "kzVCmjB8Qp-b",
        "outputId": "94821fad-0c18-4243-8c49-3b1ac2e38a3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a08e29590bd4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Section 1: Imports and Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2476\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📘 Text Analysis Tool - Complete Python Code\n",
        "\n",
        "# Section 1: Imports and Setup\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from IPython.display import display, Markdown, HTML\n",
        "import warnings\n",
        "from google.colab import files\n",
        "import io\n",
        "from datetime import datetime\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Section 2: Input Handling with Toggle Timeout\n",
        "def auto_yes(prompt):\n",
        "    import threading\n",
        "    print(prompt + \" (default: y in 3s):\", end=' ', flush=True)\n",
        "    result = {'answer': None}\n",
        "\n",
        "    def get_input():\n",
        "        result['answer'] = input().strip().lower()\n",
        "\n",
        "    t = threading.Thread(target=get_input)\n",
        "    t.daemon = True\n",
        "    t.start()\n",
        "    t.join(timeout=3)\n",
        "\n",
        "    return (result['answer'] or 'y').lower() == 'y'\n",
        "\n",
        "def get_user_input():\n",
        "    display(Markdown(\"## 👋 Welcome to the Text Insight Tool!\"))\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Type or paste text\")\n",
        "    print(\"2. Upload a text file\")\n",
        "    method = input(\"Enter 1 or 2: \")\n",
        "\n",
        "    if method == '1':\n",
        "        text = input(\"\\nPlease paste your text below:\\n\")\n",
        "    elif method == '2':\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"⚠️ No file uploaded.\")\n",
        "            return None, None, None, None\n",
        "        file = next(iter(uploaded.values()))\n",
        "        try:\n",
        "            text = io.StringIO(file.decode('utf-8')).read()\n",
        "        except:\n",
        "            print(\"❗️ Invalid or unreadable text file.\")\n",
        "            return None, None, None, None\n",
        "    else:\n",
        "        print(\"⚠️ Invalid input choice.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    if not text.strip():\n",
        "        print(\"⚠️ No input detected. Please enter or upload text.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    print(\"\\n--- Optional Feature Toggles (auto 'yes' after 3 seconds) ---\")\n",
        "    enable_spelling = auto_yes(\"Enable spelling check?\")\n",
        "    auto_correct = auto_yes(\"Enable auto-correction?\")\n",
        "    enable_formality = auto_yes(\"Enable formality detection?\")\n",
        "    enable_sentiment = auto_yes(\"Enable sentiment analysis?\")\n",
        "\n",
        "    return text, enable_spelling, auto_correct, enable_formality, enable_sentiment\n",
        "\n",
        "# Section 3: Clean Text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    personal_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n",
        "    words = [word for word in words if word not in stop_words and word not in personal_pronouns]\n",
        "    return words\n",
        "\n",
        "# Section 4: Spelling\n",
        "def check_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected = str(blob.correct())\n",
        "    original_words = set(text.split())\n",
        "    corrected_words = set(corrected.split())\n",
        "    misspelled = original_words - corrected_words\n",
        "    return corrected, list(misspelled)\n",
        "\n",
        "# Section 5: Stats\n",
        "def get_statistics(words, text):\n",
        "    total_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    sentence_count = len(nltk.sent_tokenize(text))\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words\n",
        "    word_freq = Counter(words).most_common(5)\n",
        "    return total_words, unique_words, sentence_count, avg_word_length, word_freq\n",
        "\n",
        "# Section 6: Language, Formality, Sentiment\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "def detect_formality(text):\n",
        "    informal_words = [\"lol\", \"btw\", \"gonna\", \"wanna\", \"u\", \"idk\", \"bro\", \"dunno\"]\n",
        "    contractions = [\"don't\", \"can't\", \"i'm\", \"you're\", \"won't\"]\n",
        "    text_lower = text.lower()\n",
        "    found = sum(word in text_lower for word in informal_words + contractions)\n",
        "    if found >= 3:\n",
        "        return \"Informal\"\n",
        "    elif found == 0:\n",
        "        return \"Formal\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.05:\n",
        "        sentiment = \"Positive 😊\"\n",
        "    elif polarity < -0.05:\n",
        "        sentiment = \"Negative 😞\"\n",
        "    else:\n",
        "        sentiment = \"Neutral 😐\"\n",
        "    return sentiment, polarity, blob.sentiment.subjectivity\n",
        "\n",
        "# Section 7: Visuals\n",
        "def generate_bar_chart(word_freq):\n",
        "    words, counts = zip(*word_freq)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(\"Top Frequent Words\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_wordcloud(words):\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Word Cloud\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_random_extra_charts(words):\n",
        "    choices = [\"line\", \"pie\", \"hist\"]\n",
        "    selected = random.sample(choices, k=2)\n",
        "\n",
        "    if \"line\" in selected:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(range(len(words[:10])), [len(w) for w in words[:10]], marker='o')\n",
        "        plt.title(\"Length of First 10 Words\")\n",
        "        plt.xlabel(\"Word Position\")\n",
        "        plt.ylabel(\"Word Length\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    if \"pie\" in selected:\n",
        "        freq = Counter(words).most_common(4)\n",
        "        labels, sizes = zip(*freq)\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "        plt.axis('equal')\n",
        "        plt.title(\"Most Common Words (Top 4)\")\n",
        "        plt.show()\n",
        "\n",
        "    if \"hist\" in selected:\n",
        "        lengths = [len(w) for w in words]\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.hist(lengths, bins=range(1, max(lengths)+1), color='coral', edgecolor='black')\n",
        "        plt.title(\"Distribution of Word Lengths\")\n",
        "        plt.xlabel(\"Word Length\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "\n",
        "# Section 8: Summary with Save Option\n",
        "def display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq):\n",
        "    summary = f\"\"\"\n",
        "# 🧾 Summary Dashboard\n",
        "\n",
        "## 📊 Text Statistics\n",
        "- **Total Words**: {total_words}\n",
        "- **Unique Words**: {unique_words}\n",
        "- **Sentences**: {sentence_count}\n",
        "- **Average Word Length**: {avg_word_length:.2f}\n",
        "\n",
        "## 💬 Language & Style\n",
        "- **Language**: {lang}\n",
        "- **Formality**: {formality}\n",
        "- **Spelling Issues**: {', '.join(spelling_issues) if spelling_issues else 'None'}\n",
        "\n",
        "## 😊 Sentiment\n",
        "- **Sentiment**: {sentiment}\n",
        "- **Polarity Score**: {polarity:.2f}\n",
        "- **Subjectivity**: {subjectivity:.2f}\n",
        "\n",
        "## 🔍 Top Words\n",
        "- {', '.join([f'{w} ({c})' for w, c in word_freq])}\n",
        "\"\"\"\n",
        "    display(Markdown(summary))\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"summary_{timestamp}.txt\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(summary)\n",
        "    print(f\"\\n📄 Summary saved as {filename}\")\n",
        "    display(HTML(f'<a download=\"{filename}\" href=\"files/{filename}\" target=\"_blank\">📥 Download Summary</a>'))\n",
        "    display(HTML(f'<button onclick=\"google.colab.kernel.invokeFunction(\\'notebook.exit\\', [], {{}})\">❌ Exit Tool</button>'))\n",
        "\n",
        "# Section 9: Main Runner with Return Option\n",
        "def run_text_analysis():\n",
        "    while True:\n",
        "        user_input = get_user_input()\n",
        "        if not user_input or any(v is None for v in user_input):\n",
        "            break\n",
        "\n",
        "        text, enable_spelling, auto_correct, enable_formality, enable_sentiment = user_input\n",
        "        words = clean_text(text)\n",
        "\n",
        "        if enable_spelling:\n",
        "            corrected_text, spelling_issues = check_spelling(text)\n",
        "            if auto_correct:\n",
        "                text = corrected_text\n",
        "        else:\n",
        "            spelling_issues = []\n",
        "\n",
        "        total_words, unique_words, sentence_count, avg_word_length, word_freq = get_statistics(words, text)\n",
        "        lang = detect_language(text)\n",
        "        formality = detect_formality(text) if enable_formality else 'N/A'\n",
        "        sentiment, polarity, subjectivity = analyze_sentiment(text) if enable_sentiment else ('N/A', 0.0, 0.0)\n",
        "\n",
        "        display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                        lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq)\n",
        "\n",
        "        generate_bar_chart(word_freq)\n",
        "        print(\"\\n--- Additional Charts ---\\n\")\n",
        "        generate_random_extra_charts(words)\n",
        "        print(\"\\n--- Word Cloud ---\\n\")\n",
        "        generate_wordcloud(words)\n",
        "        break\n",
        "\n",
        "# Run the tool\n",
        "run_text_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "m4ImkpUqOvmN",
        "outputId": "4f4f650a-5014-4f84-8dfc-68a091864012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-065a59e20403>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Section 1: Imports and Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2476\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📘 Text Analysis Tool - Initial Structure (Based on Full Pseudocode)\n",
        "\n",
        "# Section 1: Imports and Setup\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from IPython.display import display, Markdown\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "# Section 2: Welcome and Input Handling\n",
        "def get_user_input():\n",
        "    display(Markdown(\"## 👋 Welcome to the Text Insight Tool!\"))\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Type or paste text\")\n",
        "    print(\"2. Upload a text file\")\n",
        "    method = input(\"Enter 1 or 2: \")\n",
        "\n",
        "    if method == '1':\n",
        "        text = input(\"\\nPlease paste your text below:\\n\")\n",
        "    elif method == '2':\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"⚠️ No file uploaded.\")\n",
        "            return None, None, None, None, None, None, None\n",
        "        file = next(iter(uploaded.values()))\n",
        "        try:\n",
        "            text = io.StringIO(file.decode('utf-8')).read()\n",
        "        except:\n",
        "            print(\"❗️ Invalid or unreadable text file.\")\n",
        "            return None, None, None, None, None, None, None\n",
        "    else:\n",
        "        print(\"⚠️ Invalid input choice.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    if not text.strip():\n",
        "        print(\"⚠️ No input detected. Please enter or upload text.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    words = nltk.word_tokenize(text)\n",
        "    if len(words) < 10:\n",
        "        print(\"⚠️ Please enter at least 10 words to proceed with analysis.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # Feature toggles\n",
        "    print(\"\\n--- Optional Feature Toggles ---\")\n",
        "    enable_spelling = input(\"Enable spelling check? (y/n): \").lower() == 'y'\n",
        "    auto_correct = input(\"Enable auto-correction? (y/n): \").lower() == 'y'\n",
        "    enable_formality = input(\"Enable formality detection? (y/n): \").lower() == 'y'\n",
        "    enable_sentiment = input(\"Enable sentiment analysis? (y/n): \").lower() == 'y'\n",
        "    enable_visuals = input(\"Enable visualizations? (y/n): \").lower() == 'y'\n",
        "    enable_summary = input(\"Generate summary dashboard? (y/n): \").lower() == 'y'\n",
        "\n",
        "    return text, enable_spelling, auto_correct, enable_formality, enable_sentiment, enable_visuals, enable_summary\n",
        "\n",
        "\n",
        "# Section 3: Text Cleaning and Preprocessing\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# Section 4: Spelling Correction and Highlighting\n",
        "def check_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected = str(blob.correct())\n",
        "    original_words = set(text.split())\n",
        "    corrected_words = set(corrected.split())\n",
        "    misspelled = original_words - corrected_words\n",
        "    return corrected, list(misspelled)\n",
        "\n",
        "# Section 5: Basic Statistics\n",
        "def get_statistics(words, text):\n",
        "    total_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    sentence_count = len(nltk.sent_tokenize(text))\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words\n",
        "    word_freq = Counter(words).most_common(5)\n",
        "    return total_words, unique_words, sentence_count, avg_word_length, word_freq\n",
        "\n",
        "# Section 6: Language Detection\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        lang_code = detect(text)\n",
        "        return lang_code\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Section 7: Formality Detection (Rule-based)\n",
        "def detect_formality(text):\n",
        "    informal_words = [\"lol\", \"btw\", \"gonna\", \"wanna\", \"u\", \"idk\", \"bro\", \"dunno\"]\n",
        "    contractions = [\"don't\", \"can't\", \"i'm\", \"you're\", \"won't\"]\n",
        "    text_lower = text.lower()\n",
        "    found = sum(word in text_lower for word in informal_words + contractions)\n",
        "    if found >= 3:\n",
        "        return \"Informal\"\n",
        "    elif found == 0:\n",
        "        return \"Formal\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Section 8: Sentiment Analysis\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.05:\n",
        "        sentiment = \"Positive 😊\"\n",
        "    elif polarity < -0.05:\n",
        "        sentiment = \"Negative 😞\"\n",
        "    else:\n",
        "        sentiment = \"Neutral 😐\"\n",
        "    return sentiment, polarity, blob.sentiment.subjectivity\n",
        "\n",
        "# Section 9: Visualizations\n",
        "def generate_wordcloud(words):\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Word Cloud\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_bar_chart(word_freq):\n",
        "    words, counts = zip(*word_freq)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(\"Top Frequent Words\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "# Section 10: Summary Dashboard\n",
        "def display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq):\n",
        "    display(Markdown(f\"\"\"\n",
        "# 🧾 Summary Dashboard\n",
        "\n",
        "## 📊 Text Statistics\n",
        "- **Total Words**: {total_words}\n",
        "- **Unique Words**: {unique_words}\n",
        "- **Sentences**: {sentence_count}\n",
        "- **Average Word Length**: {avg_word_length:.2f}\n",
        "\n",
        "## 💬 Language & Style\n",
        "- **Language**: {lang}\n",
        "- **Formality**: {formality}\n",
        "- **Spelling Issues**: {', '.join(spelling_issues) if spelling_issues else 'None'}\n",
        "\n",
        "## 😊 Sentiment\n",
        "- **Sentiment**: {sentiment}\n",
        "- **Polarity Score**: {polarity:.2f}\n",
        "- **Subjectivity**: {subjectivity:.2f}\n",
        "\n",
        "## 🔍 Top Words\n",
        "- {', '.join([f'{w} ({c})' for w, c in word_freq])}\n",
        "\"\"\"))\n",
        "\n",
        "# Entry Point\n",
        "def run_text_analysis():\n",
        "    text = get_user_input()\n",
        "    if not text:\n",
        "        return\n",
        "\n",
        "    words = clean_text(text)\n",
        "    corrected_text, spelling_issues = check_spelling(text)\n",
        "    total_words, unique_words, sentence_count, avg_word_length, word_freq = get_statistics(words, text)\n",
        "    lang = detect_language(text)\n",
        "    formality = detect_formality(text)\n",
        "    sentiment, polarity, subjectivity = analyze_sentiment(text)\n",
        "\n",
        "    display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq)\n",
        "\n",
        "    generate_bar_chart(word_freq)\n",
        "    generate_wordcloud(words)\n",
        "\n",
        "# Run the tool\n",
        "run_text_analysis()\n"
      ],
      "metadata": {
        "id": "-R7Z70MM91FN",
        "outputId": "8c8bea3c-a9be-4259-ad81-2be5e30637f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## 👋 Welcome to the Text Insight Tool!"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose input method:\n",
            "1. Type or paste text\n",
            "2. Upload a text file\n",
            "Enter 1 or 2: 1\n",
            "\n",
            "Please paste your text below:\n",
            "我们正在研究自然语言处理和人工智能，这些技术在大数据和深度学习的支持下变得越来越强大。未来，大模型可能会彻底改变我们的生活方式。\n",
            "⚠️ Please enter at least 10 words to proceed with analysis.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-164145630d26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;31m# Run the tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m \u001b[0mrun_text_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-164145630d26>\u001b[0m in \u001b[0;36mrun_text_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mcorrected_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspelling_issues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_spelling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_word_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-164145630d26>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Section 3: Text Cleaning and Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'['\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m']'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "btARiNlpBQp7",
        "outputId": "fbd318dd-be96-4f1b-b830-2d04575eb7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Display welcome message to the user\n",
        "Prompt user to:\n",
        "    - Type or paste text\n",
        "    - OR upload a text/document file\n",
        "    - (Optional) Provide second text for comparison\n",
        "\n",
        "If user uploads a file:\n",
        "    - Extract text from file (.txt, .pdf, .docx, .csv)\n",
        "\n",
        "If user enters URL:\n",
        "    - Scrape text content from webpage\n",
        "\n",
        "Store text(s) for analysis\n",
        "\n",
        "Ask user to enable/disable optional features:\n",
        "    - Spelling check\n",
        "    - Auto-correction\n",
        "    - Formality detection\n",
        "    - Sentiment analysis\n",
        "    - Word cloud and charts\n",
        "    - Summary dashboard\n",
        "    - Comparison mode\n",
        "\n",
        "Clean the input text(s):\n",
        "    - Lowercase\n",
        "    - Remove punctuation\n",
        "    - Tokenize text\n",
        "    - Remove stopwords\n",
        "\n",
        "If spelling check is enabled:\n",
        "    - Highlight misspelled words\n",
        "    - Apply auto-correction (if selected)\n",
        "\n",
        "For each input text:\n",
        "    - Count total words\n",
        "    - Count unique words\n",
        "    - Count sentences\n",
        "    - Calculate average word length\n",
        "    - Identify most frequent words\n",
        "\n",
        "Detect language using langdetect\n",
        "Detect formality based on informal words, contractions, and emojis\n",
        "Perform sentiment analysis (TextBlob or VADER)\n",
        "    - Return polarity score and emoji\n",
        "\n",
        "\n",
        "If visualizations enabled:\n",
        "    - Generate word cloud\n",
        "    - Generate bar chart of word frequency\n",
        "    - (Optional) Generate pie chart of sentiment\n",
        "\n",
        "Compile results into a summary:\n",
        "    - Key statistics\n",
        "    - Tone and formality feedback\n",
        "    - Detected language\n",
        "    - Most frequent words\n",
        "    - Suggestions for improvement\n",
        "\n",
        "Display option to:\n",
        "    - Copy to clipboard\n",
        "    - Download PDF\n",
        "    - Save to history\n",
        "\n",
        "Offer options:\n",
        "    - Analyze another text\n",
        "    - Compare with a second text\n",
        "    - Exit program\n"
      ],
      "metadata": {
        "id": "nja4DjrQ4gGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnCTtuRrOsmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Initalize and Welcome the user\n",
        "\n",
        "Display welcome message: \"Welcome to the Text Insight Tool!\"\n",
        "\n",
        "Prompt user to input text via:\n",
        "    - Typing or pasting into a text box\n",
        "    - Uploading a file (.txt, .docx, .pdf, .csv)\n",
        "    - (Optional) Entering a URL\n",
        "    - (Optional) Using speech-to-text\n",
        "    - (Optional) Monitoring clipboard\n",
        "\n",
        "IF user uploads a file:\n",
        "    - Validate file type and check content\n",
        "    - IF file is unsupported or empty:\n",
        "        - Display warning: \"❗ Invalid or empty file. Please upload a valid text file.\"\n",
        "        - Return to input step\n",
        "\n",
        "IF no text is entered after any method:\n",
        "    - Display warning: \"⚠️ No input detected. Please enter or upload text before proceeding.\"\n",
        "    - Return to input step\n",
        "\n",
        "Store input text as `text_1`\n",
        "\n",
        "IF text_1 contains fewer than 10 words:\n",
        "    - Display warning: \"⚠️ Please enter at least 10 words to begin analysis.\"\n",
        "    - Return to input step\n",
        "\n",
        "IF user selects comparison:\n",
        "    - Prompt for second input (`text_2`)\n",
        "    - Run same validations for `text_2`\n",
        "\n",
        "\n",
        " 2. Show Feature Toggles\n",
        "Present toggles for the following options:\n",
        "    [x] Enable spelling check\n",
        "    [x] Show auto-correction\n",
        "    [x] Enable formality detection\n",
        "    [x] Enable sentiment analysis\n",
        "    [x] Generate visualizations\n",
        "    [x] Generate summary dashboard\n",
        "    [x] Enable text comparison (for two texts)\n",
        "\n",
        "Store toggle preferences\n",
        "\n",
        "3. Preprocess the Input Text\n",
        "For each input text:\n",
        "    - Convert to lowercase\n",
        "    - Remove punctuation\n",
        "    - Tokenize words\n",
        "    - Remove stopwords\n",
        "\n",
        "If spelling check is enabled:\n",
        "    - Identify misspelled words using TextBlob or dictionary\n",
        "    - If auto-correction enabled:\n",
        "        - Replace misspelled words with corrected versions\n",
        "\n",
        "4. Calculate Statistics\n",
        "For each text:\n",
        "    - Count total words\n",
        "    - Count unique words\n",
        "    - Count total sentences\n",
        "    - Calculate average word length\n",
        "    - Identify top N frequent words using Counter\n",
        "\n",
        "5. Analyze Language, Style, For each text:\n",
        "    - Detect language using langdetect or TextBlob\n",
        "    - Detect formality:\n",
        "        - Check for slang, contractions, emojis\n",
        "        - Classify as formal, informal, or neutral\n",
        "    - Perform sentiment analysis using TextBlob or VADER:\n",
        "        - Get polarity and subjectivity scores\n",
        "        - Classify as Positive / Neutral / Negative\n",
        "        - Add emoji feedback (😊 😐 😞)\n",
        "\n",
        "6.  Create Visualizations\n",
        "If visualizations enabled:\n",
        "    - Generate bar chart for top N frequent words\n",
        "    - Generate word cloud for whole text\n",
        "    - (Optional) Generate pie chart for sentiment proportions\n",
        "\n",
        "7.  Generate Summary Dashboard\n",
        "For each text:\n",
        "    - Combine all stats and analysis\n",
        "    - Display as a formatted markdown-style dashboard:\n",
        "        - Key metrics\n",
        "        - Sentiment summary\n",
        "        - Language/formality/spelling results\n",
        "        - Frequent/overused words\n",
        "        - Suggestions for improvement\n",
        "\n",
        "Allow options to:\n",
        "    - Download summary as PDF\n",
        "    - Copy summary to clipboard\n",
        "    - Save snapshot to \"history\"\n",
        "8. Handle Navigation & User Flow\n",
        "\n",
        "Ask user:\n",
        "    - [Run again] Analyze another text\n",
        "    - [Compare] Show side-by-side comparison (if two texts)\n",
        "    - [Exit] End the session\n",
        "\n",
        "9. Prompting Journal (Background Logging)\n",
        "Log AI-assisted tasks throughout the project:\n",
        "    - Prompt → Response → Revision → Final implementation\n",
        "    - Save entries with timestamps and section tags\n",
        "    - Store in markdown or text file for submission/reporting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WA8s67GX5l0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "text = \"I love this! It's absolutely fantastic 😊\"\n",
        "\n",
        "scores = analyzer.polarity_scores(text)\n",
        "print(scores)\n",
        "compound = scores['compound']\n",
        "\n",
        "if compound >= 0.05:\n",
        "    print(\"Sentiment: Positive 😊\")\n",
        "elif compound <= -0.05:\n",
        "    print(\"Sentiment: Negative 😞\")\n",
        "else:\n",
        "    print(\"Sentiment: Neutral 😐\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paWKCPV4Y64a",
        "outputId": "64a77441-f5c7-4e60-e0cb-fcaad921f988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.264, 'pos': 0.736, 'compound': 0.855}\n",
            "Sentiment: Positive 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XsODohLUY3ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO6aXMgEYTPu",
        "outputId": "e10ca60e-e389-49fb-d9f6-d1b650c88982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfU267lMY3Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import streamlit\n",
        "    print(\"streamlit is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"streamlit is not installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG_UUxHTAw1m",
        "outputId": "35f6725b-413d-4dc6-df45-db9dcbaa1715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streamlit is not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import matplotlib\n",
        "    print(\"matplotlib is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"matplotlib is not installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlfsevW-AXvH",
        "outputId": "9f5fd032-6105-49ce-b310-e826092bc5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matplotlib is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import re\n",
        "    print(\"re is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"re is not installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-UDDmzzAKSx",
        "outputId": "424e880a-fde3-481b-9d63-7d4f589552ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Eft0GgoAEcn",
        "outputId": "5fe3b2be-154e-47a3-fcab-9abf6505d2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txWlI2H3cy5d",
        "outputId": "a4363b2c-d578-46ab-b915-d7f2f93abd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textblob is already installed.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import textblob\n",
        "    print(\"textblob is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"textblob is not installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_8_kNcE_3Fn",
        "outputId": "3bb4525a-80de-46d8-e41a-efd7123fc5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import nltk\n",
        "    print(\"nltk is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"nltk is not installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA6F3xkD-MAf",
        "outputId": "d0093093-c96b-4c47-be59-f111284e13d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk is already installed.\n"
          ]
        }
      ]
    }
  ]
}