{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1uJnLi/CYeWTIfxtWiwiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricLe1404/text_analyse_tool/blob/main/Text_Analyse_Tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìò Text Analysis Tool - Initial Structure (Based on Full Pseudocode)\n",
        "\n",
        "# Section 1: Imports and Setup\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from IPython.display import display, Markdown\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "# Section 2: Welcome and Input Handling\n",
        "def get_user_input():\n",
        "    display(Markdown(\"## üëã Welcome to the Text Insight Tool!\"))\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Type or paste text\")\n",
        "    print(\"2. Upload a text file\")\n",
        "    method = input(\"Enter 1 or 2: \")\n",
        "\n",
        "    if method == '1':\n",
        "        text = input(\"\\nPlease paste your text below:\\n\")\n",
        "    elif method == '2':\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"‚ö†Ô∏è No file uploaded.\")\n",
        "            return None, None, None, None, None, None, None\n",
        "        file = next(iter(uploaded.values()))\n",
        "        try:\n",
        "            text = io.StringIO(file.decode('utf-8')).read()\n",
        "        except:\n",
        "            print(\"‚ùóÔ∏è Invalid or unreadable text file.\")\n",
        "            return None, None, None, None, None, None, None\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Invalid input choice.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    if not text.strip():\n",
        "        print(\"‚ö†Ô∏è No input detected. Please enter or upload text.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    words = nltk.word_tokenize(text)\n",
        "    if len(words) < 10:\n",
        "        print(\"‚ö†Ô∏è Please enter at least 10 words to proceed with analysis.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # Feature toggles\n",
        "    print(\"\\n--- Optional Feature Toggles ---\")\n",
        "    enable_spelling = input(\"Enable spelling check? (y/n): \").lower() == 'y'\n",
        "    auto_correct = input(\"Enable auto-correction? (y/n): \").lower() == 'y'\n",
        "    enable_formality = input(\"Enable formality detection? (y/n): \").lower() == 'y'\n",
        "    enable_sentiment = input(\"Enable sentiment analysis? (y/n): \").lower() == 'y'\n",
        "    enable_visuals = input(\"Enable visualizations? (y/n): \").lower() == 'y'\n",
        "    enable_summary = input(\"Generate summary dashboard? (y/n): \").lower() == 'y'\n",
        "\n",
        "    return text, enable_spelling, auto_correct, enable_formality, enable_sentiment, enable_visuals, enable_summary\n",
        "\n",
        "\n",
        "# Section 3: Text Cleaning and Preprocessing\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[' + string.punctuation + ']', '', text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# Section 4: Spelling Correction and Highlighting\n",
        "def check_spelling(text):\n",
        "    blob = TextBlob(text)\n",
        "    corrected = str(blob.correct())\n",
        "    original_words = set(text.split())\n",
        "    corrected_words = set(corrected.split())\n",
        "    misspelled = original_words - corrected_words\n",
        "    return corrected, list(misspelled)\n",
        "\n",
        "# Section 5: Basic Statistics\n",
        "def get_statistics(words, text):\n",
        "    total_words = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    sentence_count = len(nltk.sent_tokenize(text))\n",
        "    avg_word_length = sum(len(word) for word in words) / total_words\n",
        "    word_freq = Counter(words).most_common(5)\n",
        "    return total_words, unique_words, sentence_count, avg_word_length, word_freq\n",
        "\n",
        "# Section 6: Language Detection\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        lang_code = detect(text)\n",
        "        return lang_code\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Section 7: Formality Detection (Rule-based)\n",
        "def detect_formality(text):\n",
        "    informal_words = [\"lol\", \"btw\", \"gonna\", \"wanna\", \"u\", \"idk\", \"bro\", \"dunno\"]\n",
        "    contractions = [\"don't\", \"can't\", \"i'm\", \"you're\", \"won't\"]\n",
        "    text_lower = text.lower()\n",
        "    found = sum(word in text_lower for word in informal_words + contractions)\n",
        "    if found >= 3:\n",
        "        return \"Informal\"\n",
        "    elif found == 0:\n",
        "        return \"Formal\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Section 8: Sentiment Analysis\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.05:\n",
        "        sentiment = \"Positive üòä\"\n",
        "    elif polarity < -0.05:\n",
        "        sentiment = \"Negative üòû\"\n",
        "    else:\n",
        "        sentiment = \"Neutral üòê\"\n",
        "    return sentiment, polarity, blob.sentiment.subjectivity\n",
        "\n",
        "# Section 9: Visualizations\n",
        "def generate_wordcloud(words):\n",
        "    wc = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Word Cloud\")\n",
        "    plt.show()\n",
        "\n",
        "def generate_bar_chart(word_freq):\n",
        "    words, counts = zip(*word_freq)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(words, counts, color='skyblue')\n",
        "    plt.title(\"Top Frequent Words\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "# Section 10: Summary Dashboard\n",
        "def display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq):\n",
        "    display(Markdown(f\"\"\"\n",
        "# üßæ Summary Dashboard\n",
        "\n",
        "## üìä Text Statistics\n",
        "- **Total Words**: {total_words}\n",
        "- **Unique Words**: {unique_words}\n",
        "- **Sentences**: {sentence_count}\n",
        "- **Average Word Length**: {avg_word_length:.2f}\n",
        "\n",
        "## üí¨ Language & Style\n",
        "- **Language**: {lang}\n",
        "- **Formality**: {formality}\n",
        "- **Spelling Issues**: {', '.join(spelling_issues) if spelling_issues else 'None'}\n",
        "\n",
        "## üòä Sentiment\n",
        "- **Sentiment**: {sentiment}\n",
        "- **Polarity Score**: {polarity:.2f}\n",
        "- **Subjectivity**: {subjectivity:.2f}\n",
        "\n",
        "## üîç Top Words\n",
        "- {', '.join([f'{w} ({c})' for w, c in word_freq])}\n",
        "\"\"\"))\n",
        "\n",
        "# Entry Point\n",
        "def run_text_analysis():\n",
        "    text = get_user_input()\n",
        "    if not text:\n",
        "        return\n",
        "\n",
        "    words = clean_text(text)\n",
        "    corrected_text, spelling_issues = check_spelling(text)\n",
        "    total_words, unique_words, sentence_count, avg_word_length, word_freq = get_statistics(words, text)\n",
        "    lang = detect_language(text)\n",
        "    formality = detect_formality(text)\n",
        "    sentiment, polarity, subjectivity = analyze_sentiment(text)\n",
        "\n",
        "    display_summary(total_words, unique_words, sentence_count, avg_word_length,\n",
        "                    lang, formality, spelling_issues, sentiment, polarity, subjectivity, word_freq)\n",
        "\n",
        "    generate_bar_chart(word_freq)\n",
        "    generate_wordcloud(words)\n",
        "\n",
        "# Run the tool\n",
        "run_text_analysis()\n"
      ],
      "metadata": {
        "id": "-R7Z70MM91FN",
        "outputId": "74d10b45-f8d3-46ad-88ed-5aa4faf5b680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## üëã Welcome to the Text Insight Tool!"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose input method:\n",
            "1. Type or paste text\n",
            "2. Upload a text file\n",
            "Enter 1 or 2: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-164145630d26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;31m# Run the tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m \u001b[0mrun_text_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-164145630d26>\u001b[0m in \u001b[0;36mrun_text_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;31m# Entry Point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_text_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-164145630d26>\u001b[0m in \u001b[0;36mget_user_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPlease paste your text below:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "btARiNlpBQp7",
        "outputId": "fbd318dd-be96-4f1b-b830-2d04575eb7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Display welcome message to the user\n",
        "Prompt user to:\n",
        "    - Type or paste text\n",
        "    - OR upload a text/document file\n",
        "    - (Optional) Provide second text for comparison\n",
        "\n",
        "If user uploads a file:\n",
        "    - Extract text from file (.txt, .pdf, .docx, .csv)\n",
        "\n",
        "If user enters URL:\n",
        "    - Scrape text content from webpage\n",
        "\n",
        "Store text(s) for analysis\n",
        "\n",
        "Ask user to enable/disable optional features:\n",
        "    - Spelling check\n",
        "    - Auto-correction\n",
        "    - Formality detection\n",
        "    - Sentiment analysis\n",
        "    - Word cloud and charts\n",
        "    - Summary dashboard\n",
        "    - Comparison mode\n",
        "\n",
        "Clean the input text(s):\n",
        "    - Lowercase\n",
        "    - Remove punctuation\n",
        "    - Tokenize text\n",
        "    - Remove stopwords\n",
        "\n",
        "If spelling check is enabled:\n",
        "    - Highlight misspelled words\n",
        "    - Apply auto-correction (if selected)\n",
        "\n",
        "For each input text:\n",
        "    - Count total words\n",
        "    - Count unique words\n",
        "    - Count sentences\n",
        "    - Calculate average word length\n",
        "    - Identify most frequent words\n",
        "\n",
        "Detect language using langdetect\n",
        "Detect formality based on informal words, contractions, and emojis\n",
        "Perform sentiment analysis (TextBlob or VADER)\n",
        "    - Return polarity score and emoji\n",
        "\n",
        "\n",
        "If visualizations enabled:\n",
        "    - Generate word cloud\n",
        "    - Generate bar chart of word frequency\n",
        "    - (Optional) Generate pie chart of sentiment\n",
        "\n",
        "Compile results into a summary:\n",
        "    - Key statistics\n",
        "    - Tone and formality feedback\n",
        "    - Detected language\n",
        "    - Most frequent words\n",
        "    - Suggestions for improvement\n",
        "\n",
        "Display option to:\n",
        "    - Copy to clipboard\n",
        "    - Download PDF\n",
        "    - Save to history\n",
        "\n",
        "Offer options:\n",
        "    - Analyze another text\n",
        "    - Compare with a second text\n",
        "    - Exit program\n"
      ],
      "metadata": {
        "id": "nja4DjrQ4gGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Initalize and Welcome the user\n",
        "\n",
        "Display welcome message: \"Welcome to the Text Insight Tool!\"\n",
        "\n",
        "Prompt user to input text via:\n",
        "    - Typing or pasting into a text box\n",
        "    - Uploading a file (.txt, .docx, .pdf, .csv)\n",
        "    - (Optional) Entering a URL\n",
        "    - (Optional) Using speech-to-text\n",
        "    - (Optional) Monitoring clipboard\n",
        "\n",
        "IF user uploads a file:\n",
        "    - Validate file type and check content\n",
        "    - IF file is unsupported or empty:\n",
        "        - Display warning: \"‚ùó Invalid or empty file. Please upload a valid text file.\"\n",
        "        - Return to input step\n",
        "\n",
        "IF no text is entered after any method:\n",
        "    - Display warning: \"‚ö†Ô∏è No input detected. Please enter or upload text before proceeding.\"\n",
        "    - Return to input step\n",
        "\n",
        "Store input text as `text_1`\n",
        "\n",
        "IF text_1 contains fewer than 10 words:\n",
        "    - Display warning: \"‚ö†Ô∏è Please enter at least 10 words to begin analysis.\"\n",
        "    - Return to input step\n",
        "\n",
        "IF user selects comparison:\n",
        "    - Prompt for second input (`text_2`)\n",
        "    - Run same validations for `text_2`\n",
        "\n",
        "\n",
        " 2. Show Feature Toggles\n",
        "Present toggles for the following options:\n",
        "    [x] Enable spelling check\n",
        "    [x] Show auto-correction\n",
        "    [x] Enable formality detection\n",
        "    [x] Enable sentiment analysis\n",
        "    [x] Generate visualizations\n",
        "    [x] Generate summary dashboard\n",
        "    [x] Enable text comparison (for two texts)\n",
        "\n",
        "Store toggle preferences\n",
        "\n",
        "3. Preprocess the Input Text\n",
        "For each input text:\n",
        "    - Convert to lowercase\n",
        "    - Remove punctuation\n",
        "    - Tokenize words\n",
        "    - Remove stopwords\n",
        "\n",
        "If spelling check is enabled:\n",
        "    - Identify misspelled words using TextBlob or dictionary\n",
        "    - If auto-correction enabled:\n",
        "        - Replace misspelled words with corrected versions\n",
        "\n",
        "4. Calculate Statistics\n",
        "For each text:\n",
        "    - Count total words\n",
        "    - Count unique words\n",
        "    - Count total sentences\n",
        "    - Calculate average word length\n",
        "    - Identify top N frequent words using Counter\n",
        "\n",
        "5. Analyze Language, Style, For each text:\n",
        "    - Detect language using langdetect or TextBlob\n",
        "    - Detect formality:\n",
        "        - Check for slang, contractions, emojis\n",
        "        - Classify as formal, informal, or neutral\n",
        "    - Perform sentiment analysis using TextBlob or VADER:\n",
        "        - Get polarity and subjectivity scores\n",
        "        - Classify as Positive / Neutral / Negative\n",
        "        - Add emoji feedback (üòä üòê üòû)\n",
        "\n",
        "6.  Create Visualizations\n",
        "If visualizations enabled:\n",
        "    - Generate bar chart for top N frequent words\n",
        "    - Generate word cloud for whole text\n",
        "    - (Optional) Generate pie chart for sentiment proportions\n",
        "\n",
        "7.  Generate Summary Dashboard\n",
        "For each text:\n",
        "    - Combine all stats and analysis\n",
        "    - Display as a formatted markdown-style dashboard:\n",
        "        - Key metrics\n",
        "        - Sentiment summary\n",
        "        - Language/formality/spelling results\n",
        "        - Frequent/overused words\n",
        "        - Suggestions for improvement\n",
        "\n",
        "Allow options to:\n",
        "    - Download summary as PDF\n",
        "    - Copy summary to clipboard\n",
        "    - Save snapshot to \"history\"\n",
        "8. Handle Navigation & User Flow\n",
        "\n",
        "Ask user:\n",
        "    - [Run again] Analyze another text\n",
        "    - [Compare] Show side-by-side comparison (if two texts)\n",
        "    - [Exit] End the session\n",
        "\n",
        "9. Prompting Journal (Background Logging)\n",
        "Log AI-assisted tasks throughout the project:\n",
        "    - Prompt ‚Üí Response ‚Üí Revision ‚Üí Final implementation\n",
        "    - Save entries with timestamps and section tags\n",
        "    - Store in markdown or text file for submission/reporting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WA8s67GX5l0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "text = \"I love this! It's absolutely fantastic üòä\"\n",
        "\n",
        "scores = analyzer.polarity_scores(text)\n",
        "print(scores)\n",
        "compound = scores['compound']\n",
        "\n",
        "if compound >= 0.05:\n",
        "    print(\"Sentiment: Positive üòä\")\n",
        "elif compound <= -0.05:\n",
        "    print(\"Sentiment: Negative üòû\")\n",
        "else:\n",
        "    print(\"Sentiment: Neutral üòê\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paWKCPV4Y64a",
        "outputId": "64a77441-f5c7-4e60-e0cb-fcaad921f988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.264, 'pos': 0.736, 'compound': 0.855}\n",
            "Sentiment: Positive üòä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XsODohLUY3ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO6aXMgEYTPu",
        "outputId": "e10ca60e-e389-49fb-d9f6-d1b650c88982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfU267lMY3Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import streamlit\n",
        "    print(\"streamlit is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"streamlit is not installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG_UUxHTAw1m",
        "outputId": "35f6725b-413d-4dc6-df45-db9dcbaa1715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streamlit is not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import matplotlib\n",
        "    print(\"matplotlib is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"matplotlib is not installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlfsevW-AXvH",
        "outputId": "9f5fd032-6105-49ce-b310-e826092bc5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matplotlib is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import re\n",
        "    print(\"re is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"re is not installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-UDDmzzAKSx",
        "outputId": "424e880a-fde3-481b-9d63-7d4f589552ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "re is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Eft0GgoAEcn",
        "outputId": "5fe3b2be-154e-47a3-fcab-9abf6505d2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txWlI2H3cy5d",
        "outputId": "a4363b2c-d578-46ab-b915-d7f2f93abd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textblob is already installed.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import textblob\n",
        "    print(\"textblob is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"textblob is not installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_8_kNcE_3Fn",
        "outputId": "3bb4525a-80de-46d8-e41a-efd7123fc5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import nltk\n",
        "    print(\"nltk is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"nltk is not installed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA6F3xkD-MAf",
        "outputId": "d0093093-c96b-4c47-be59-f111284e13d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk is already installed.\n"
          ]
        }
      ]
    }
  ]
}